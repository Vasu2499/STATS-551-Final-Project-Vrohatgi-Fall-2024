group_by(year) %>%
summarise(total_cancer_incidence = sum(Total_Count, na.rm = TRUE), .groups = "drop")
# Plotting cancer incidence over time
ggplot(total_cancer_by_year, aes(x = year, y = total_cancer_incidence)) +
geom_line(color = "blue") +
geom_point(color = "red") +
labs(title = "Total Cancer Incidence Over Time (All States)",
x = "Year",
y = "Total Cancer Incidence") +
theme_minimal()
ggsave(
filename = "cancer incidence over time.pdf",
device = "pdf",
width = 12,
height = 8,
units = "in"
)
# Summarizing Total_Count by State
state_summary <- cleaned_data %>%
group_by(States) %>%
summarise(
avg_total_count = mean(Total_Count, na.rm = TRUE),
total_total_count = sum(Total_Count, na.rm = TRUE),
.groups = "drop"
)
# Viewing the summarized data
head(state_summary)
# Calculating total cancer incidence and total population by state
state_proportion <- cleaned_data %>%
group_by(States) %>%
summarise(
total_cancer = sum(Total_Count, na.rm = TRUE),
total_population = sum(Total_Population, na.rm = TRUE),
cancer_proportion = total_cancer / total_population,
.groups = "drop"
)
# Plotting the cancer incidence proportion by state
ggplot(state_proportion, aes(x = reorder(States, cancer_proportion), y = cancer_proportion)) +
geom_bar(stat = "identity", fill = "lightblue") +
coord_flip() +
labs(title = "Cancer Incidence as Proportion of Population by State",
x = "State",
y = "Cancer Incidence Proportion") +
theme_minimal()
ggsave(
filename = "cancer incidence proportion.pdf",
device = "pdf",
width = 12,
height = 8,
units = "in"
)
# Define the predictors and response variable
predictors <- c("avg_max_aqi", "avg_moderate_days", "avg_unhealthy_days", "avg_very_unhealthy_days", "avg_days_pm2_5")
response <- "Total_Count"
# Subset the data for predictors and response
data_for_model <- cleaned_data %>%
select(States, year, all_of(predictors), response) %>%
na.omit()
# Create a matrix for predictors (X) and a vector for the response (y)
X <- as.matrix(data_for_model[, predictors])
y <- data_for_model[, response]
# Define the number of observations (N), time periods (T), and predictors (K)
N <- nrow(X)
T <- length(unique(data_for_model$year))
K <- ncol(X)
# Plot the observed vs predicted values (for the first posterior sample)
plot(y, y_pred_all[, 1], main="Observed vs Predicted", xlab="Observed", ylab="Predicted")
# install.packages(c("tidyverse", "ggplot2", "zoo"))
library(tidyverse)
library(ggplot2)
library(zoo)
library(readr)
library(stringr)
library(forcats)
library(dplyr)
library(purrr)
library(tidyr)
raw_data <- read.csv("final_dataset_consolidated.csv")
colnames(raw_data)
cleaned_data <- raw_data %>%
mutate(across(
starts_with("avg_") |
c("Total_Count", "Total_Population", "year"),
~ as.numeric(.)
)) %>%
filter(
!is.na(Total_Count) & Total_Count >= 0,
!is.na(Total_Population) & Total_Population > 0,
!is.na(year) & year >= 1999
) %>%
# Check for outliers by identifying extreme values
filter(
avg_max_aqi <= 500,  # Cap the AQI values at reasonable levels (e.g., 500)
avg_days_with_aqi <= 365  # Cap days of AQI at 365, prevent unrealistic values
) %>%
# Remove unnecessary columns with blank or redundant data
select(
-contains("CharacteristicName"),
-contains("ResultMeasure"),
-contains("ResultValueTypeName"),
-contains("PrecisionValue"),
-contains("DataQuality.BiasValue"),
-contains("ResultDepthHeightMeasure")
) %>%
group_by(States, year) %>%
summarise(across(everything(), \(x) mean(x, na.rm = TRUE)))
# Summary of the dataset
summary(cleaned_data)
# Check the structure and column types
str(cleaned_data)
# Calculate summary statistics for numeric columns
cleaned_data %>%
summarise(across(where(is.numeric), list(mean = ~mean(.x, na.rm = TRUE),
median = ~median(.x, na.rm = TRUE),
sd = ~sd(.x, na.rm = TRUE))))
# AQI trends over the years for each state
ggplot(cleaned_data, aes(x = year, y = avg_max_aqi, color = States)) +
geom_line() +
theme_minimal() +
labs(title = "Average Max AQI Over Years by State",
x = "Year", y = "Average Max AQI") +
theme_minimal()
ggsave(
filename = "detailed_aqi_plot.pdf",
device = "pdf",
width = 12,
height = 8,
units = "in"
)
# Average AQI by State
state_avg_aqi <- cleaned_data %>%
group_by(States) %>%
summarise(avg_aqi = mean(avg_max_aqi, na.rm = TRUE)) %>%
arrange(desc(avg_aqi))
ggplot(state_avg_aqi, aes(x = reorder(States, avg_aqi), y = avg_aqi)) +
geom_bar(stat = "identity", fill = "steelblue") +
coord_flip() +
theme_minimal() +
labs(title = "Average AQI by State", x = "State", y = "Average AQI")
ggsave(
filename = "AQI by state.pdf",
device = "pdf",
width = 12,
height = 8,
units = "in"
)
# Scatter plot of AQI vs. Total Population
ggplot(cleaned_data, aes(x = Total_Population, y = avg_max_aqi)) +
geom_point(aes(color = States), alpha = 0.6) +
scale_x_continuous(labels = scales::comma) +
theme_minimal() +
labs(title = "Average AQI vs. Total Population", x = "Total Population", y = "Average AQI")
ggsave(
filename = "AQI vs. Total Population.pdf",
device = "pdf",
width = 12,
height = 8,
units = "in"
)
library(ggplot2)
library(dplyr)
# Split states into two groups
state_groups <- cleaned_data %>%
mutate(group = ifelse(States %in% unique(States)[1:25], "Group 1", "Group 2"))
# Function to create the plot for each group
create_facet_plot <- function(data, title_suffix) {
ggplot(data, aes(x = year, y = avg_max_aqi, group = States)) +
geom_line(color = "steelblue", size = 0.8) +
facet_wrap(~ States, ncol = 5, nrow = 5, scales = "free_y") +
theme_minimal(base_size = 12) +
labs(
title = paste("State-Specific AQI Trends Over Years", title_suffix),
x = "Year", y = "Average Max AQI"
) +
scale_y_continuous(
breaks = function(x) pretty(x, n = 3)
) +
scale_x_continuous(
breaks = seq(
min(cleaned_data$year, na.rm = TRUE),
max(cleaned_data$year, na.rm = TRUE),
by = 5
)
) +
theme(
strip.text = element_text(size = 10, face = "bold"),
axis.text.x = element_text(size = 10, angle = 30, hjust = 1),
axis.text.y = element_text(size = 8),
plot.title = element_text(size = 16, face = "bold"),
panel.spacing = unit(1.5, "lines"),
plot.margin = margin(10, 10, 10, 10)
)
}
# Generate plots for Group 1 and Group 2
plot_group1 <- create_facet_plot(state_groups %>% filter(group == "Group 1"), "(Group 1)")
plot_group2 <- create_facet_plot(state_groups %>% filter(group == "Group 2"), "(Group 2)")
# Save both plots to a single PDF
pdf("state_aqi_trends_split_adjusted.pdf", width = 18, height = 14)
dev.off()
library(ggplot2)
library(dplyr)
# Sort states by median unhealthy days
cleaned_data <- cleaned_data %>%
mutate(States = reorder(States, avg_unhealthy_days, median, na.rm = TRUE))
# Create the boxplot with all 50 states
ggplot(cleaned_data, aes(x = States, y = avg_unhealthy_days)) +
geom_boxplot(aes(fill = as.factor(cut(avg_unhealthy_days, breaks = 5))),
outlier.color = "red", outlier.size = 1.2) +
scale_fill_manual(values = c("lightgreen", "darkgreen", "yellow", "orange", "red")) +
coord_flip() +
theme_minimal(base_size = 12) +
labs(
title = "Unhealthy AQI Days by State",
x = "State",
y = "Unhealthy Days",
fill = "Avg Unhealthy Days"
) +
theme(
axis.text.y = element_text(size = 6, vjust = 1, hjust = 1),
axis.text.x = element_text(size = 10, angle = 90, hjust = 1),
axis.ticks.y = element_blank(),
axis.title.x = element_text(size = 8),
axis.title.y = element_text(size = 8),
plot.title = element_text(size = 10, face = "bold", hjust = 0.5),
legend.position = "right",
legend.key.width = unit(1, "cm"),
legend.title = element_text(size = 10),
legend.text = element_text(size = 10),
plot.margin = margin(15, 15, 15, 30),
scale_y_continuous(breaks = seq(0, max(cleaned_data$avg_unhealthy_days), by = 2))
)
ggsave(
filename = "boxplot with all 50 states.pdf",
device = "pdf",
width = 12,
height = 8,
units = "in"
)
# Ensure that the data is ungrouped
cleaned_data <- cleaned_data %>% ungroup()
# Select only numeric columns
numeric_data <- cleaned_data %>%
select(where(is.numeric))
# Compute the correlation matrix
cor_matrix <- cor(numeric_data, use = "complete.obs")
# Print the correlation matrix
print(cor_matrix)
# Visualize the correlation matrix using a heatmap
heatmap(cor_matrix,
main = "Correlation Matrix",
col = colorRampPalette(c("blue", "white", "red"))(100),
scale = "none",
margins = c(8, 8))
ggsave(
filename = "correlation matrix.pdf",
device = "pdf",
width = 12,
height = 8,
units = "in"
)
# Selecting relevant numeric columns for correlation
correlation_data <- cleaned_data %>%
select(Total_Count, avg_max_aqi, avg_moderate_days, avg_unhealthy_days, avg_very_unhealthy_days, avg_days_pm2_5)
# Calculating correlation matrix for selected variables
cor_matrix <- cor(correlation_data, use = "complete.obs")
# Viewing the correlation matrix
print(cor_matrix)
# Summing Total_Count across all states by year
total_cancer_by_year <- cleaned_data %>%
group_by(year) %>%
summarise(total_cancer_incidence = sum(Total_Count, na.rm = TRUE), .groups = "drop")
# Plotting cancer incidence over time
ggplot(total_cancer_by_year, aes(x = year, y = total_cancer_incidence)) +
geom_line(color = "blue") +
geom_point(color = "red") +
labs(title = "Total Cancer Incidence Over Time (All States)",
x = "Year",
y = "Total Cancer Incidence") +
theme_minimal()
ggsave(
filename = "cancer incidence over time.pdf",
device = "pdf",
width = 12,
height = 8,
units = "in"
)
# Calculating total cancer incidence and total population by state
state_proportion <- cleaned_data %>%
group_by(States) %>%
summarise(
total_cancer = sum(Total_Count, na.rm = TRUE),
total_population = sum(Total_Population, na.rm = TRUE),
cancer_proportion = total_cancer / total_population,
.groups = "drop"
)
# Plotting the cancer incidence proportion by state
ggplot(state_proportion, aes(x = reorder(States, cancer_proportion), y = cancer_proportion)) +
geom_bar(stat = "identity", fill = "lightblue") +
coord_flip() +
labs(title = "Cancer Incidence as Proportion of Population by State",
x = "State",
y = "Cancer Incidence Proportion") +
theme_minimal()
ggsave(
filename = "cancer incidence proportion.pdf",
device = "pdf",
width = 12,
height = 8,
units = "in"
)
# Define the predictors and response variable
predictors <- c("avg_max_aqi", "avg_moderate_days", "avg_unhealthy_days", "avg_very_unhealthy_days", "avg_days_pm2_5")
response <- "Total_Count"
# Subset the data for predictors and response
data_for_model <- cleaned_data %>%
select(States, year, all_of(predictors), response) %>%
na.omit()
# Create a matrix for predictors (X) and a vector for the response (y)
X <- as.matrix(data_for_model[, predictors])
y <- data_for_model[, response]
# Define the number of observations (N), time periods (T), and predictors (K)
N <- nrow(X)
T <- length(unique(data_for_model$year))
K <- ncol(X)
# Prepare the data list for STAN
stan_data <- list(
N = N,
T = T,
K = K,
X = X,
y = y
)
library(rstan)
rstan_options(auto_write = TRUE)
options(mc.cores = parallel::detectCores())
model <- stan_model("finalmodel.stan")
y <- as.vector(cleaned_data$Total_Count)
N <- length(y)
state_levels <- unique(cleaned_data$States)
state_index <- as.integer(factor(cleaned_data$States, levels = state_levels))
year_index <- cleaned_data$year
X <- cleaned_data[, c("avg_max_aqi", "avg_days_with_aqi", "avg_good_days",
"avg_moderate_days", "avg_unhealthy_days")]
stan_data <- list(
N = N,
K = ncol(X),
y = y,
X = X,
state_index = state_index,
year_index = year_index,
S = length(state_levels)
)
fit <- sampling(model, data = stan_data, iter = 4000, chains = 4)
print(fit)
# Summarize the posterior
summary(fit)
beta_sample <- samples$beta[1, ]
# Convert tibble to a numeric matrix
X_numeric <- as.matrix(X)
# Ensure all columns are numeric
X_numeric <- apply(X_numeric, 2, as.numeric)
str(X_numeric)
y_pred <- X_numeric %*% beta_sample
# For the full posterior predictive computation
y_pred_all <- apply(samples$beta, 1, function(beta_sample) {
X_numeric %*% beta_sample  # Prediction for each posterior sample
})
str(y_pred_all)
# Plot the observed vs predicted values (for the first posterior sample)
plot(y, y_pred_all[, 1], main="Observed vs Predicted", xlab="Observed", ylab="Predicted")
abline(a=0, b=1, col="red")  # Add identity line
mse <- mean((y_obs - y_pred_mean)^2)
cat("Mean Squared Error:", mse, "\n")
# Calculate posterior predictive values using alpha samples
y_pred_all <- apply(samples$beta, 1, function(beta_sample) {
X_numeric %*% beta_sample
})
dim(y_pred_all)
length(samples$alpha)
# Correct element-wise multiplication with exp(samples$alpha)
# Use matrix multiplication or broadcasting where needed
y_pred_all <- sweep(y_pred_all, MARGIN = 2, STATS = exp(samples$alpha), FUN = "*")
# Continue with posterior predictive checks and plots
y_pred_mean <- rowMeans(y_pred_all)
y_pred_lower <- apply(y_pred_all, 1, quantile, probs = 0.025)
y_pred_upper <- apply(y_pred_all, 1, quantile, probs = 0.975)
# Plot observed vs predicted values with credible intervals
plot(y_obs, y_pred_mean,
xlab = "Observed Total Count",
ylab = "Predicted Total Count",
main = "Posterior Predictive Check",
pch = 16, col = "blue")
abline(0, 1, col = "red", lwd = 2)
arrows(x0 = y_obs, y0 = y_pred_lower, x1 = y_obs, y1 = y_pred_upper,
angle = 90, code = 3, length = 0.05, col = "gray")
# Residuals analysis
residuals <- y_obs - y_pred_mean
hist(residuals, main = "Residuals Distribution", xlab = "Residuals")
# Model Evaluation
mse <- mean(residuals^2)
cat("Mean Squared Error (MSE):", mse, "\n")
rmse <- sqrt(mse)
cat("Root Mean Squared Error (RMSE):", rmse, "\n")
sst <- sum((y_obs - mean(y_obs))^2)
sse <- sum(residuals^2)
r_squared <- 1 - (sse / sst)
cat("R-squared (R²):", r_squared, "\n")
# Posterior Predictive P-value (PPP)
ppp <- mean(abs(y_pred_all - mean(y_pred_all)) >= abs(y_obs - mean(y_obs)))
cat("Posterior Predictive P-value (PPP):", ppp, "\n")
beta_sample <- samples$beta[1, ]
# Convert tibble to a numeric matrix
X_numeric <- as.matrix(X)
# Ensure all columns are numeric
X_numeric <- apply(X_numeric, 2, as.numeric)
str(X_numeric)
y_pred <- X_numeric %*% beta_sample
# For the full posterior predictive computation
y_pred_all <- apply(samples$beta, 1, function(beta_sample) {
X_numeric %*% beta_sample  # Prediction for each posterior sample
})
str(y_pred_all)
pdf(file = "1.pdf", width = 12, height = 8)
# Plot the observed vs predicted values (for the first posterior sample)
plot(y, y_pred_all[, 1], main="Observed vs Predicted", xlab="Observed", ylab="Predicted")
abline(a=0, b=1, col="red")  # Add identity line
dev.off()
pdf(file = "2.pdf", width = 12, height = 8)
# Plot the histogram of the predictions for the first observation
hist(y_pred_all[1, ], main="Posterior Predictive Distribution for Observation 1", xlab="Prediction")
dev.off()
# Visualizing the posterior distributions for coefficients
posterior_plot <- as.data.frame(samples$beta) %>%
ggplot(aes(x = V1)) +
geom_density() +
labs(title = "Posterior Distribution of Beta1 (avg_max_aqi)", x = "Beta1", y = "Density") +
theme_minimal()
print(posterior_plot)
ggsave(
filename = "posterior distributions for coefficients.pdf",
device = "pdf",
width = 12,
height = 8,
units = "in"
)
# Predictions for all posterior samples
y_pred_all <- apply(samples$beta, 1, function(beta_sample) {
X_numeric %*% beta_sample
})
y_obs <- cleaned_data$Total_Count
str(y_obs)
stopifnot(length(y_obs) == nrow(X))
y_pred_mean <- rowMeans(y_pred_all)
y_pred_lower <- apply(y_pred_all, 1, quantile, probs = 0.025)
y_pred_upper <- apply(y_pred_all, 1, quantile, probs = 0.975)
pdf(file = "3.pdf", width = 12, height = 8)
plot(y_obs, y_pred_mean,
xlab = "Observed Total Count",
ylab = "Predicted Total Count",
main = "Posterior Predictive Check",
pch = 16, col = "blue")
abline(0, 1, col = "red", lwd = 2)
arrows(x0 = y_obs, y0 = y_pred_lower, x1 = y_obs, y1 = y_pred_upper,
angle = 90, code = 3, length = 0.05, col = "gray")
dev.off()
residuals <- y_obs - y_pred_mean
hist(residuals, main = "Residuals Distribution", xlab = "Residuals")
mse <- mean((y_obs - y_pred_mean)^2)
cat("Mean Squared Error:", mse, "\n")
# Calculate posterior predictive values using alpha samples
y_pred_all <- apply(samples$beta, 1, function(beta_sample) {
X_numeric %*% beta_sample
})
dim(y_pred_all)
length(samples$alpha)
# Correct element-wise multiplication with exp(samples$alpha)
# Use matrix multiplication or broadcasting where needed
y_pred_all <- sweep(y_pred_all, MARGIN = 2, STATS = exp(samples$alpha), FUN = "*")
# Continue with posterior predictive checks and plots
y_pred_mean <- rowMeans(y_pred_all)
y_pred_lower <- apply(y_pred_all, 1, quantile, probs = 0.025)
y_pred_upper <- apply(y_pred_all, 1, quantile, probs = 0.975)
pdf(file = "4.pdf", width = 12, height = 8)
# Plot observed vs predicted values with credible intervals
plot(y_obs, y_pred_mean,
xlab = "Observed Total Count",
ylab = "Predicted Total Count",
main = "Posterior Predictive Check",
pch = 16, col = "blue")
abline(0, 1, col = "red", lwd = 2)
arrows(x0 = y_obs, y0 = y_pred_lower, x1 = y_obs, y1 = y_pred_upper,
angle = 90, code = 3, length = 0.05, col = "gray")
dev.off()
# Residuals analysis
residuals <- y_obs - y_pred_mean
hist(residuals, main = "Residuals Distribution", xlab = "Residuals")
# Model Evaluation
mse <- mean(residuals^2)
cat("Mean Squared Error (MSE):", mse, "\n")
rmse <- sqrt(mse)
cat("Root Mean Squared Error (RMSE):", rmse, "\n")
sst <- sum((y_obs - mean(y_obs))^2)
sse <- sum(residuals^2)
r_squared <- 1 - (sse / sst)
cat("R-squared (R²):", r_squared, "\n")
# Posterior Predictive P-value (PPP)
ppp <- mean(abs(y_pred_all - mean(y_pred_all)) >= abs(y_obs - mean(y_obs)))
cat("Posterior Predictive P-value (PPP):", ppp, "\n")
samples <- extract(fit)
pdf(file = "traceplot_fit.pdf", width = 8, height = 6)
traceplot(fit)
dev.off()
